\section{Sorting on GPU}

We measure the performance of our multiprocessing and MPI mergesort algorithms on the Carbonate GPU partition. Each node
is equipped with two Intel 6248 2.5 GHz 20-core CPUs, four NVIDIA Tesla V100 PCle 32 GB GPUs, one 1.92 TB solid-state
% drive, and 768 GB of RAM. We execute our merge sorts with randomly generated arrays of up to 10^7 integer elements. Note
that memory constraints allowed us to experiment with arrays of up to BOGO integer elements.

Multiprocessing merge sort was executed on 1 to 24 processes by using all available processes on the Carbonate partition. Message-passing merge sort was executed on 1, 2, 4, and 8 nodes by using one core on each node for MPI processes. 

\section{Performance Comparison}

All performance comparison done here will be on data from the Carbonate partition. 

\subsection{Time}

Using our results from the Carbonate partition, we analyze and graph the resulting times from selected runs. 

\begin{table}
    \caption{This is an example.}
    \label{tab:times-sort}
    \centering
    % \includegraphics{https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-sort-type.png}
    % \include{a.tex}
\end{table}
% ![Table : Times for different sorts](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-sort-type.png)

We can analyze time in two ways. First, we analyze algorithm performance based on increasing size of arrays. Some algorithms differ within themselves (for example, two multiprocessing merge sorts might use different numbers of processes). However, we can account for this variation by aggregating all the values and showing error bands around the lines that are plotted. 

\begin{figure}
    \centering
    % \includegraphics[width=0.5\textwidth,height=\textheight]{https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-size-sort.png}
    \caption{Times for different sorts}
    \label{fig:times-sort}
\end{figure}
% ![Fig : Times for different sorts](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-size-sort.png){width="50%"}


The figure illustrates the average run time for arrays of up to size $10^7$ for each different sort type (sequential merge sort, multiprocessing merge sort, and MPI merge sort.) 

The general behavior displayed in the figure can be summarized in the following points:
\begin{enumerate}

    \item For array sizes of up to around 2 million, the multiprocessing mergesort, on average, 
    is quicker than the MPI mergesort. The overhead of MPI contributes significantly more to the 
    overall sort time for smaller arrays, since the creation of processes takes longer than the 
    creation of threads. However, the MPI mergesort is consistently the fastest sort type when arrays 
    are sufficiently large enough that the time reduction from parallelism on individual nodes is enough 
    to offset the additional costs of communication and thread creation. 

    \item The MPI sort displays the lowest average increase in time relative for array size, with a mean value of 1.708 seconds/million numbers. In other words, when the array size increases by an additional one million elements, the MPI sort time increases by an average of 1.708 seconds. The multiprocessing sort underperforms this significantly, with a mean increase of 2.706 seconds per additional million numbers. The sequential sort has a mean increase of 8.754 seconds per additional million numbers. 
\end{enumerate}

Second, we analyze algorithm performance based on processes used. We will keep the size of the array at a constant value and, for each number of processes, compare the times for each sorting algorithm. 

![Table x: Time by process for size=10000, mp](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/p-time-mp-10000.png)

![Table x: Time by process for size=10000, mpi](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/p-time-mpi-10000.png)

![Fig 2: Time by process for size=10000](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-p-sort-10000.png){width="50%"}

For arrays of size 10000, increasing MPI parallelism is actually a positive factor for time. This aligns with what we see above, where MPI mergesort initally performs worse than multiprocessing mergesort on small arrays. 

![Table x: Time by process for size=1e7, mp](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/p-time-mp-1e7.png)

![Table x: Time by process for size=1e7, mp](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/p-time-mpi-1e7.png)

![Fig 3: Time by process for size=10000000](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/images/time-by-p-sort-10000000.png)

For arrays of size $10^7$, we can see in figure 3 that increasing paralellism reduces runtimes for both algorithms. 

\subsection{Speedup}

Speedup is defined as $\frac{T_s}{T_p}$, where $T_s$ is the compute time of the sequential algorithm and $T_p$ is the compute time of the parallel algorithm. 

\subsection{Efficiency}

\section{Conclusion}

- MPI is advantageous to use when data is large enough
- further speedup can be obtained by using MPI I/O operations. 

\subsection{Source Code}

The source code is located in GitHub at the following location:

- <https://github.com/cloudmesh/cloudmesh-mpi/tree/main/examples/sort>

We distinguish the following important files:

- [night.py](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/night.py)

\subsection{Installation}

\subsubsection{ENV3 (for macOS)}

 bash
$ python -m venv ~/ENV3
$ source ~/ENV3/bin/activate


\subsubsection{Clone}

To download our code, please follow the instructions:

bash
$ mkdir cm
$ cd cm
$ git clone git@github.com:cloudmesh/cloudmesh-mpi.git


\subsubsection{Verification}

Go to the [Github](https://github.com/cloudmesh/cloudmesh-mpi) to verify
that the correct repository has been cloned.

\subsubsection{Updating}

1. Update local version

   \begin{verbatim} bash
   git pull
   \end{verbatim}

2. To add file (only do once)

   \begin{verbatim}
   git add filename
   \end{verbatim}

3. Once file is changed, do

   \begin{verbatim}
   git commit -m "this is my comment" filename
   \end{verbatim}

4. Remote upload

   \begin{verbatim}
   git push
   \end{verbatim}

\subsection{Running the Program}

Run the program using the command

\begin{verbatim}
./mpi_run.py --user={username} --node={node} --sort={mpi_mergesort}--size={size} --repeat={repeat} --id={id}
\end{verbatim}

Please read the Input sectionfor specific documentation on each option.

\subsection{Overview}

This project uses Python to implement an MPI mergesort algorithm (linked
[here](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/night.py)).
The algorithm is then run and evaluated in
[mpi_run.py](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/mpi_run.py).

\subsubsection{Input

*mpi_run.py* has several inputs.

*user*: specify the username of whoever is running the program\
*node*: name of computer node on which program is being run\
*sort*: type of sort that is being run. Use "mpi_mergesort".\
*size*: size of array to sort\
*repeat*: number of times to run sorting algorithm. Final time will be
the average of all runs.\
*id*: specifies the merging algorithm that is used to merge the arrays
once they've been sent from one process to another, also known as the
merge algorithm. Each id number corresponds to a specific algorithm.\
0: built-in Python *sorted*\
1: sequential merge\
2: adaptive merge

For example, if John would like to run the MPI mergesort on his
Raspberry Pi 4, and he would like to use an array of size 200, repeating
10 times, and use an adaptive merge to combine the the arrays between
processes, he would use the command

\begin{verbatim}
./mpi_run.py --user=john --node=pi4 --sort=mpi_mergesort --size=200 --repeat=10 --id=2
\end{verbatim}

\subsubsection{Mergesort

The unsorted array is generated on rank 0. Since the unsorted array must
be split into smaller subarrays, *sub_size* is calculated. *sub_size* is
the size of each subarray. It is equal to the size of the unsorted array
divided by the number of processors, since each processor must be sent a
subarry. Note that *n*, the size of the unsorted array, must be evenly
divisible by the total number of processors.

Once the subarrays have been distributed using the Scatter command, they
are sorted on each processor using the built-in Python. This sort
defaults to Timsort.

In order to merge the sorted subarrays, we can visualize the processors
as being set up in a binary tree, where each parent has two children.
One important note: in this situation, the left child also functions as
the parent node. We can then split the tree into two sections: left
children and right children. Because we are using a binary tree, we know
that the number of left and right children will always be equal.
Therefore, we can create a variable *split* that splits the set of
processors in half.

If the rank of the processor is in the second half (between *split* and
*split* \* 2), then it will send its sorted subarray to its "left"
partner to be merged. Otherwise, if the rank of the processor is in the
first half (between 0 and *split*), it will recieve a sorted subarray
from its "right" partner and then merge it with the subarray that it
currently contains. When a subarray is sent, it is sent to the processor
with rank *rank - split*, ensuring that the processor that it is sent to
has a rank between 0 and *split*. This guarantees that each subarray
that is sent gets sent to a merging processor. Similarly, when a
subarray is received, it is received from a processor with rank *rank +
split*, ensuring that the subarray is a sorted array to be merged. This
mapping guarantees a unique pairing between left and right child.

Once received, the subarrays are merged together. The merging algorithm
can be defined by the user, and will be referred to as the **merge
algorithm**. There are currently three merging algorithms that can be
used. 1. **sequential_merge_fast**, a "fast" merge that simply combines
the two arrays using the built-in Python *sorted* function. Note that
sorting algorithms can also be used to merge. 2.
**sequential_merge_python**, a merge that uses the well-known technique
of appending the smaller of two array elements to a third array 3.
**adaptive_merge**, a custom sort algorithm that takes advantage of
pre-existing order in the sequence. It is described in more detail
[here](sectionadaptive).

Then, each individual send/recieve operates as following:

1. Left child sends sorted subarray
2. Right child allocates memory for recieving subarray from left child
   (*local_tmp*)
3. Right child allocates memory for array to store merged result
   (*local_result*)
4. Right child receives subarray
5. Right child merges received subarray with its own subarray
6. Right child assigns *local_arr* to point to *local_result*

This loop continues until the tree reaches the height that guarantees us
a single sorted list. Each time, the number of nodes is halved (since
two have been merged into one).

\subsubsection{Output

The output of the program is generated and logged in
[mpi_experiment.py](hhttps://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/mpi_experiment.py).
This file runs the sort according to user specifications. Three of the
most important things it does: 1. The specific algorithm is run *repeat*
times. Note that the larger *repeat* is, the more accurate the final
mean time will be. The mean time is not calculated here. Rather, all
times from each repeat will be outputted. 2. Maps id numbers to merge
types. It's important to be able to differentiate between data outputted
by each merge type, since we'd like to examine the differences. However,
we'd also like to keep the titles/inputs concise, which is why id
numbers are used. Each id number is translated to its merge type in the
program. 3. Times the runs. Running the sort is done by a bash command,
so by starting a timer, running the command, and stopping the timer, we
can get the full amount of time it took to run the sort. This is a more
accurate measure than placing timers inside of the actual sort program
(night.py), since it is able to time the MPI initialization, which
timers inside the program wouldn't be able to do.

\subsubsection{Final Run Method

This is the outermost run program, i.e. the one that a user will run.
See the above Input \section{for an example of how to run this.
[mpi_run.py](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/mpi_run.py)
generates a log file and runs
[mpi_experiment.py](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/mpi_experiment.py).

\subsection{adaptive}

This sort takes advantage of pre-existing sorted subsequences in two
arrays to merge them together. It is designed to merge two sorted arrays
together. Given two arrays, called *left* and *right*, we first
calculate the min and max values in both.

\begin{verbatim}
   l_min = left[0] 
   r_min = right[0]
   l_max = left[len(left) - 1]
   r_max = right[len(right) - 1]
\end{verbatim}

We can also assume that the minimum of left will always be smaller than
the minimum of right (if not, we can just swap them).

The sort splits into three scenarios based on *l_min*, *r_min*, *l_max*,
and *r_max*:

\subsubsection{No overlap}

This would be if *l_max* \<= *r_min*. In this case, a visual
representation of the arrays could look like this.

\begin{verbatim}
left
------------
                  ----------
                     right
\end{verbatim}

where the arrays are entirely disjoint. Then, we can just concatenate
the two and return.

\subsubsection{Some overlap}

This would be if *l_max* were between *r_min* and *r_max*, or *l_max* \>
*r_min* and *l_max* \< *r_max*.

A visual representation:

\begin{verbatim}
left
--------------
         -----------------
                     right
\end{verbatim}

Then, we want to split both arrays into two parts:

\begin{verbatim}
left
---------|-----
          -----|------------
                     right
\end{verbatim}

If we take the divider in *left* and call it *l_idx*, we can see that
everything to the left of *l_idx* is completely separate from *right*
with no overlap. Similarly, everything to the right of the divider in
*right* has no overlap with any of *left*.

Then, how do we define these dividers? Note that the visual is slightly
misleading: the arrays are not continuous spans of numbers, rather, they
are individual scattered points that span from a min to a max. Then, if
we want to find *l_idx* such that everything to the left of *l_idx*
doesn't overlap with *right*, we want to find the index of the smallest
element in *left* that is greater than *r_min*. Since every element to
the left would then be less than or equal to *r_min*, that point would
then be the *l_idx* we are looking for.

Similarly, we want to search in *right* for some *r_idx* where
everything to the right of *r_idx* has no overlap with *left*. Then, we
want to find the index of the largest element in *right* that is less
than *l_max*. Since every element to the right would then be greater or
equal to *l_max*, that point would be *r_idx*.

From here, binary search is employed to find such points. Read the
Binary search section for more information.

Once *l_idx* and *r_idx* are found, we can then separate out the
disjoint parts.

\begin{verbatim}
   left_sorted = left[:l_idx]
   right_sorted = right[r_idx + 1:]
\end{verbatim}

The remaining parts can then be concatenated and sorted. This sort uses
the Python builtin sort.

\begin{verbatim}
   unsorted = np.concatenate((left[l_idx:], right[:r_idx+1]))
   unsorted = np.array(sorted(unsorted))
\end{verbatim}

Finally, the left sorted portion, the now-sorted middle portion, and the
right sorted portion are concatenated and returned.

\subsubsection{Complete overlap}

This would be if *l_max* \>= *r_max*.

A visual representation:

\begin{verbatim}
left
-------------------
      ------
      right
\end{verbatim}

Then, we want to split *left* into three parts:

\begin{verbatim}
left
------|------|-------
       ------
      right
\end{verbatim}

Let the leftmost divider in *left* be *left_sorted_min*, and the
rightmost divider in *left* be *left_sorted_max.* Then, everything to
the left of *left_sorted_min* has no overlap with *right*, and
everything to the right of *left_sorted_max* has no overlap with
*right*.

To find *left_sorted_min*, we want to find the smallest element in
*left* that is larger than *r_min*. To find *left_sorted_max*, we want
to find the largest element in *left* that is smaller that *r_max*. We
can do this using binary search - the specific algorithm is described in
the Binary search section.

We can isolate the sorted, separate ends with

\begin{verbatim}
left_sorted_min = left[:l_idx_min]
left_sorted_max = left[l_idx_max + 1:]
\end{verbatim}

Then, we can merge the middle of *left* and all of *right* together and
sort the resulting array using the builtin Python sorted.

\begin{verbatim}
unsorted = np.concatenate((right, left[l_idx_min:l_idx_max + 1]))
unsorted = np.array(sorted(unsorted))
\end{verbatim}

Finally, the left end, the sorted middle section, and the right end can
be concatenated together and returned.

\subsubsection{Binary search

This binary search takes in three parameters: 1. *arr*: the array in
which to search 2. *val*: the value with which to compare, and 3.
*ineq*: the comparision with which to compare the result to val

Expressed in words: If *ineq* = "\<", we are searching for the largest
value in *arr* that is less than *val*.

If *ineq* = "\>", we are searching for the smallest value in *arr* that
is greater than *val*.

The actual algorithm is classic. Calculate a middle value and perform a
boolean-evaluating statement. If true, take the lower half of the array.
Otherwise, take the upper half of the array. However, note that if the
array is increasing, this will only work if *ineq* = "\>", since true
will always lead to the upper half of the array.

Instead of writing two different binary searches, we make two crucial
changes to the function: 1. **Using eval statements.** Instead of
hardcoding a \< or a \> in the if-statement, we can instead construct a
statement to check.

\begin{verbatim}
to_check = str(arr[mid]) + ineq + str(val)
eval(to_check)
\end{verbatim}

Doing this allows us to have both "\<" and "\>" statements, depending on
which one is inputted.

2. **Reversing the array.** If the *ineq* = "\<" and the array is
   increasing, this function will never work, since we always take the
   upper half of the array when the check statement is true. However,
   if the array is decreasing, then taking the upper half works: those
   values are the ones that are less than *val*, and the ones we want
   to continue to search in. At the end, we can reverse the array, and
   flip the answer to the reversed index, i.e. 2 would become the 2nd
   index from the end.

\begin{verbatim}
arr = np.flip(arr)
return len(arr) - l - 1
\end{verbatim}

\subsection{Analyzing the Results}

Analysis of the data takes place in
[mpi.ipynb](https://github.com/cloudmesh/cloudmesh-mpi/blob/main/examples/sort/mpi.ipynb).

The function *get_data* extracts data from the logfiles. It specifically
gets the number of processes, the time, the size of the array, the id,
and name, and the tag from each line of output in each log file. This
data is then formatted using Pandas DataFrames.

We can then plot the data using *plot_benchmark_by_size*. This function
generates the name, title, and graphs the given data by whichever
specific attribute the user would like.

![Performance comparision of different sorting
Algorithms](https://github.com/cloudmesh/cloudmesh-mpi/raw/main/doc/chapters/images/mpi-mpi_mergesort-0-1-2.png){width="
50%"}

The above figure shows the relationship between increasing size of array
and time of sorting for three different merging algorithms. Recall that
0 is the Python builtin sort, 1 is the iterative merge, and 2 is the
adaptive sort. The sizes of the arrays used are \[100, 1000, 10000,
100000, 1000000, 5000000, 7000000, 10000000\]. We can see that for the
first half of sizes, all three sorts appear to perform roughly the same.
However, once we pass that point, the builtin Python sort performs much
better than the other two. We can also see that while the adaptive merge
always performs better than the iterative merge, it is still slower than
the builtin Python sort. Lastly, we can observe the very low variability
of the Python builtin sort, even as array size increases. Both the other
two sorts have a great deal of variation, but the builtin sort stays
relatively precise.
\end{comment}